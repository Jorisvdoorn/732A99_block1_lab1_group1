---
title: "group1_block1_lab1_report_732A99"
author: |
  | **GROUP 01**
  | Bayu Brahmantio  - baybr878
  | Joris van Doorn - jorva845
  | Jose Jaime Mendez Flores
date: "`r format(Sys.time(), '%d %B %Y')`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=9, fig.height = 4.1) 
library(readxl)
library(kknn)
```

# Assignment 1 - Spam classification with nearest neighbors

# 1. setting working directory and importing data
install.packages("readxl")
library(readxl)

setwd("/Users/joris/Documents/Joris/Link?ping University/Semester 1/Period 2/732A99 - Machine Learning/Lab1")

#spam_data <- read_excel("spambase.xlsx")
spam_data <- read_xlsx("spambase.xlsx")

# dividing the data into training and test sets (50/50)
n=dim(spam_data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=spam_data[id,]
test=spam_data[-id,]

# Run the model
spam_logistic <- glm(train$Spam~., family = binomial, data = train)
predicted <- predict(spam_logistic, test, type = "response")

# 2. Classify on a 0.5 split
predicted_class <- as.numeric(predicted > 0.5)
# confusion matrix
table(predicted_class, test$Spam)

# 3. Classify on a 0.8 split
predicted_class <- as.numeric(predicted > 0.8)
# confusion matrix
table(predicted_class, test$Spam)

# 4. K-nearest neighbor with K = 30
install.packages("kknn")
library(kknn)

predicted <- kknn(train$Spam~., train, test, k = 30)
predicted_class <- as.numeric(predicted$fitted.values > 0.5)
# confusion matrix
table(predicted_class, test$Spam)

# 5. K-nearest neighbor with K = 1
predicted <- kknn(train$Spam~., train, test, k = 1)
predicted_class <- as.numeric(predicted$fitted.values > 0.5)
# confusion matrix
table(predicted_class, test$Spam)


# ------------------------------------------------------------------------------------------


# Assignment 03  
### Test your function on data set swiss available in the standard R repository. Fertility should be Y and all other variables should be X. Nfolds should be X.  

**Report the resulting plot and interpret it**  

* On the X axis of the plot are shown the number of features used at a time. The Y axis shows the minimal Mean Squared Error (MSE) obtained in the different folds.
* As we increase the number of features used in our model, the MSE reduces. 
* We can also see in the 1 variable case, that there should be specifically one feature that reduces drastically the MSE. 
* Another interesting characteristic of this plot, is that the min MSE was obtained with 4 features, instead of 5.  

**Report the optimal subset of features and comment whether it is reasonable that these specific features have largest impact on the target**  

The best result was obtained using features 1 (Agriculture), 3 (Education), 4 (Catholic) and 5 (Infant Mortality). This result, makes sense for variables 1, 3 and 4 as these variables are related with education and lifestyle. But variable 5 does not make sense, because infant mortality should not have any effect on fertility. 

```{r echo = FALSE}
#ASSIGNMENT 3

data(swiss)

#linear regression
mylin=function(X,Y, Xpred){
  Xpred1 = cbind(1,Xpred)
  #NotMISSING: check formulas for linear regression and compute beta
  x1 = cbind(1,X)
  beta <- ((solve(t(x1) %*% x1)) %*% t(x1)) %*% (Y)
  Res = Xpred1 %*% beta
  return(Res)
}

myCV=function(X,Y,Nfolds){
  n=length(Y)
  p=ncol(X)
  set.seed(12345)
  ind=sample(n,n)
  X1=X[ind,]
  Y1=Y[ind]
  sF=floor(n/Nfolds)
  MSE=numeric(2^p-1)
  Nfeat=numeric(2^p-1)
  Features=list()
  curr=0
  
  #we assume 5 features.
  
  for (f1 in 0:1)
    for (f2 in 0:1)
      for(f3 in 0:1)
        for(f4 in 0:1)
          for(f5 in 0:1){
            model= c(f1,f2,f3,f4,f5)
            if (sum(model)==0) next()
            SSE=0
            
            for (k in 1:Nfolds){
              #not MISSING: compute which indices should belong to current fold
              test_index <- if (k == Nfolds){
                (sF*(k-1)+1):n
                } 
              else{
                (sF*(k-1)+1):((k * sF))
              }
              #MISSING: implement cross-validation for model with features in "model" and iteration i.
              #MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
              if(sum(model)==1){
                Ypred <- mylin(X[,which(model==1)][-test_index], Y[-test_index], X[,which(model==1)][test_index])
              }
              else{
                Ypred <- mylin(X[,which(model==1)][-test_index,], Y[-test_index], X[,which(model==1)][test_index,])
              }
              Yp <- Y[test_index]
              SSE=SSE+sum((Ypred-Yp)^2)
            }
            curr=curr+1
            MSE[curr]=SSE/n
            Nfeat[curr]=sum(model)
            Features[[curr]]=model
            
          }
  
  #MISSING: plot MSE against number of features
  nr_features <- c()
  for (i in 1:length(Features)){
    nr_features[i] <- sum(Features[[i]]) 
  }
  plot(nr_features, MSE, xlab = "Number of features")
  #plot End
  i=which.min(MSE)
  return(list(CV=MSE[i], Features=Features[[i]]))
}


myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
```


# ---------------------------------------------------------------------


## Assignment 4. Linear regression and regularization
Plot of `Protein` vs `Moisture`:  
```{r, echo = FALSE, warning = FALSE}
#setwd("D:/LiU/Machine Learning/Lab 1")
#df = read_excel("tecator.xlsx")
#plot(df$Protein, df$Moisture, xlab = "Protein", ylab = "Moisture")
```

There is a clear linear relationship between `Protein` and `Moisture` and it could be described well by a linear model.   


**Consider model $M_i$ in which Moisture is normally distributed, and the expected Moisture is a polynomial function of Protein including the polynomial terms up to power $i$.**   
Probabilistic model for $M_i$:

$$M_i \sim N(\sum_{j=0}^{i} w_j x^j, \; \sigma^2),\; i \in \{1, 2, 3, 4, 5, 6\} \\$$
We use mean of squared error (MSE) because we want to make our model $\hat{M_i}$ to be as close as possible to the training data $M_i$. So the lower the value of MSE, the better the model fits the training data. That way, we can select the best model by looking at the lowest MSE value.   
MSE for $M_i,\;i= 1,2, ..., 6$ :  
```{r, echo = FALSE, warning = FALSE}
#X = df$Protein
#Y = df$Moisture

#for (i in 2:6){
#  X = cbind(X, (df$Protein)^i)
#}

#n = nrow(df)
#train_index = 1:(floor(0.5*n))
#X_train = as.matrix(X[train_index, ])
#X_val = as.matrix(X[-train_index, ])

# Y_train = as.matrix(Y[train_index])
# Y_val = as.matrix(Y[-train_index])
# 
# MSE_train = c()
# MSE_val = c()
# for (i in 1:6){
#   Y = Y_train
#   X = X_train[,1:i]
# 
#   lm1 = lm(Y ~ X)
# 
#   y_hat_train = cbind(1, X_train[,1:i]) %*% lm1$coefficients
#   MSE_train[i] =  mean((y_hat_train - Y_train)^2)
# 
#   y_hat_val = cbind(1, X_val[,1:i]) %*% lm1$coefficients
#   MSE_val[i] = mean((y_hat_val - Y_val)^2)
# 
# }
# 
# make_plot = function(){
#   plot(x = 1:6, y = MSE_val, type = "b", ylim = c(min(MSE_train)-1,max(MSE_val)+1), pch = 19, col = "red", xlab = "i", ylab = "MSE")
#   lines(x = 1:6, y = MSE_train, type = "b", pch = 19, col = "blue")
#   legend(x = 1, y = 42, c("train", "validation"), lwd = c(1, 1), col = c("blue", "red"), pch = c(19, 19))
# }
# make_plot()
```

According to the plot, if we look at MSE values on training set, $M_6$ is the best model. However, if we look at the validation MSE, $M_1$ has the smallest errors. This can be explained by bias-variance trade-off. Models with higher degree of polynomial have lower bias but higher variance, so the model would overfit the training data and resulting in higher prediction errror when presented with validation data. If the models have lower degree of polynomial, it would underfit the data and have higher training MSE. However, due to the lower variance, they generalize better than more complex models hence the lower validation MSE.   

**Variable selection of a linear model in which Fat is response and Channel1-Channel100 are predictors by using stepAIC:**   
```{r, echo = FALSE, warning = FALSE}
# df_new = df[,c(-1, -103, -104)]
# lm2 = lm(Fat~., data = df_new)
# lm3 = stepAIC(lm2, trace = FALSE)
# cat("Number of variables selected: ", length(names(lm3$model))-1)
```
There are 63 variables selected out of 100 variables.   

**Ridge regression:**   
```{r, echo = FALSE, warning = FALSE}
# covariates = scale(lm3$model[,-1])
# response = scale(lm3$model[,1])
# ridge_model = glmnet(x = as.matrix(covariates), 
#                 y = response, alpha = 0,
#                 family = "gaussian")
# plot(ridge_model, xvar="lambda", label=TRUE)
```

**LASSO:**   
```{r, echo = FALSE, warning = FALSE}
# lasso_model = glmnet(x = as.matrix(covariates), 
#                      y = response, alpha = 1,
#                      family = "gaussian")
# plot(lasso_model, xvar="lambda", label=TRUE)
```

Based on two plots above, we can see that the ridge regression still use all of 63 variables as the value of $\lambda$ increased, although the value of the coefficients got smaller but not reaching 0. In the LASSO model, some coefficients reached to 0 even when $\lambda$ is still not relatively big. When $\ln{\lambda} = -6$ or $\lambda \approx 0.002$, there were 10 variables left that have nonzero coefficient. In conclusion, LASSO models use only several variables as the value of $\lambda$ grows while ridge regression models use all variables.   

**Cross-validation to find the optimal LASSO model:**   
```{r, echo = FALSE, warning = FALSE}
# lambdas = c(0, exp(seq(-6, 1, by = 0.1)))
# lasso_model = glmnet(x = as.matrix(covariates), 
#                      y = response, alpha = 1,
#                      family = "gaussian",
#                      lambda = lambdas)
# plot(lasso_model, xvar="lambda", label=TRUE)
# 
# cv_fit = cv.glmnet(x = as.matrix(covariates), 
#                    y = response, 
#                    alpha = 1, 
#                    lambda = lambdas)
# cv_fit
# plot(cv_fit$lambda, cv_fit$cvm, xlab = "lambda", ylab = "MSE")
```

The value of  $\lambda$ chosen is $\lambda = 0$ which is the same as using all 63 variables. In conclusion, the best model for predicting `Fat` is by using 63 variables that were selected by `stepAIC`. As seen on the plot, the value of MSE increased as the value of $\lambda$ increased too.   

# Appendix

